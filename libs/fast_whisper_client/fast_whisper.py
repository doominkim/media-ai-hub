# pip install faster-whisper í•„ìš”
import boto3
from faster_whisper import WhisperModel
from io import BytesIO
import tempfile
import torch
import torchaudio
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple
from silero_vad import load_silero_vad, read_audio, get_speech_timestamps

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ
WHISPER_MODEL = "base" # ë˜ëŠ” "small", "medium", "large" ì¤‘ ì„ íƒ

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì•ˆì „í•œ ì´ˆê¸°í™”
device = "cpu"  # ê¸°ë³¸ê°’ì„ CPUë¡œ ì„¤ì •
compute_type = "int8"
whisper_model = None

def initialize_whisper_model():
    """Whisper ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global device, compute_type, whisper_model
    
    try:
        # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if torch.cuda.is_available():
            try:
                # GPU í…ŒìŠ¤íŠ¸: ê°„ë‹¨í•œ CUDA ì—°ì‚° ìˆ˜í–‰
                test_tensor = torch.tensor([1.0]).cuda()
                _ = test_tensor + 1
                device = "cuda"
                compute_type = "float16"
                print(f"ğŸš€ GPU ê°ì§€ë¨: {torch.cuda.get_device_name()}")
                print(f"ğŸ”¥ CUDA ë²„ì „: {torch.version.cuda}")
            except Exception as cuda_error:
                print(f"âš ï¸  CUDA í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨, CPU ëª¨ë“œë¡œ ëŒ€ì²´: {cuda_error}")
                device = "cpu"
                compute_type = "int8"
        else:
            print("ğŸ“± CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            device = "cpu"
            compute_type = "int8"
            
        # Whisper ëª¨ë¸ ë¡œë“œ ì‹œë„ (cuDNN í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì •)
        try:
            # CTranslate2ì˜ cuDNN ì„¤ì • ì¡°ì •
            os.environ["CT2_CUDNN_ALLOW_FALLBACK"] = "1"
            os.environ["CT2_CUDA_ALLOCATOR"] = "cuda_malloc_async"
            
            whisper_model = WhisperModel(
                WHISPER_MODEL, 
                device=device, 
                compute_type=compute_type,
                # CTranslate2 ì„¤ì • ì¶”ê°€
            )
            print(f"âœ… Fast-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {WHISPER_MODEL} on {device}")
        except Exception as model_error:
            print(f"âš ï¸  {device.upper()} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„: {model_error}")
            device = "cpu"
            compute_type = "int8"
            whisper_model = WhisperModel(WHISPER_MODEL, device=device, compute_type=compute_type)
            print(f"âœ… Fast-Whisper ëª¨ë¸ CPU ë¡œë“œ ì„±ê³µ: {WHISPER_MODEL}")
            
    except Exception as e:
        print(f"ğŸ’¥ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
        device = "cpu"
        compute_type = "int8"
        whisper_model = WhisperModel(WHISPER_MODEL, device=device, compute_type=compute_type)
        print("ğŸ”§ ê¸°ë³¸ CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ëª¨ë¸ ì´ˆê¸°í™” ì‹¤í–‰
initialize_whisper_model()

# Silero VAD ëª¨ë¸ ë¡œë“œ
torch.set_num_threads(1)  # ê³µì‹ ë¬¸ì„œ ê¶Œì¥ì‚¬í•­
vad_model = load_silero_vad()

def extract_voice_segments(audio_path: str, min_duration: float = 0.5) -> list:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ìŒì„± êµ¬ê°„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        min_duration: ìµœì†Œ ìŒì„± êµ¬ê°„ ê¸¸ì´ (ì´ˆ)
        
    Returns:
        list: [(ì‹œì‘ ì‹œê°„, ì¢…ë£Œ ì‹œê°„), ...] í˜•ì‹ì˜ ìŒì„± êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ (16kHzë¡œ ë¦¬ìƒ˜í”Œë§)
        wav = read_audio(audio_path, sampling_rate=16000)
        
        # ì˜¤ë””ì˜¤ ì •ê·œí™”
        wav = wav / (torch.max(torch.abs(wav)) + 1e-8)
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
        audio_duration = len(wav) / 16000
        print(f"ğŸµ ì˜¤ë””ì˜¤ ë¶„ì„: ê¸¸ì´={audio_duration:.2f}ì´ˆ")
        
        # ìŒì„± êµ¬ê°„ ê°ì§€ (ë” ê´€ëŒ€í•œ ì„¤ì •)
        speech_timestamps = get_speech_timestamps(wav, vad_model, 
                                                threshold=0.4,  # 0.6ì—ì„œ 0.4ë¡œ ë‚®ì¶¤
                                                sampling_rate=16000,
                                                min_speech_duration_ms=500,  # 1000ì—ì„œ 500ìœ¼ë¡œ ì¤„ì„ (0.5ì´ˆ)
                                                min_silence_duration_ms=200,  # 400ì—ì„œ 200ìœ¼ë¡œ ì¤„ì„
                                                window_size_samples=1024,
                                                speech_pad_ms=50,  # 20ì—ì„œ 50ìœ¼ë¡œ ì¦ê°€ (ë” ë§ì€ íŒ¨ë”©)
                                                return_seconds=True)
        
        print(f"ğŸ” VAD ê²°ê³¼: {len(speech_timestamps)}ê°œ ìŒì„± êµ¬ê°„ ê°ì§€")
        
        # êµ¬ê°„ ë³‘í•© ë¡œì§
        merged_segments = []
        if speech_timestamps:
            current_start, current_end = speech_timestamps[0]['start'], speech_timestamps[0]['end']
            for ts in speech_timestamps[1:]:
                start, end = ts['start'], ts['end']
                if start - current_end < 1.0:  # 500msì—ì„œ 1.0ì´ˆë¡œ ì¦ê°€ (ë” ì ê·¹ì ì¸ ë³‘í•©)
                    current_end = end
                else:
                    if current_end - current_start >= min_duration:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                        merged_segments.append((current_start, current_end))
                    current_start, current_end = start, end
            
            # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
            if current_end - current_start >= min_duration:
                merged_segments.append((current_start, current_end))
                
        print(f"ğŸ“Š ë³‘í•© í›„: {len(merged_segments)}ê°œ ìµœì¢… ìŒì„± êµ¬ê°„")
        for i, (start, end) in enumerate(merged_segments):
            print(f"   êµ¬ê°„ {i+1}: {start:.2f}ì´ˆ ~ {end:.2f}ì´ˆ ({end-start:.2f}ì´ˆ)")
        
        return merged_segments
        
    except Exception as e:
        print(f"âš ï¸ VAD ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        # VAD ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (fallback ì²˜ë¦¬ëŠ” í˜¸ì¶œí•˜ëŠ” ê³³ì—ì„œ)
        return []

def extract_audio_segment(audio_path: str, start_time: float, end_time: float) -> str:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ íŠ¹ì • êµ¬ê°„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        audio_path: ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        start_time: ì‹œì‘ ì‹œê°„ (ì´ˆ)
        end_time: ì¢…ë£Œ ì‹œê°„ (ì´ˆ)
        
    Returns:
        str: ì¶”ì¶œëœ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ì„ì‹œ ê²½ë¡œ
    """
    # ì˜¤ë””ì˜¤ ë¡œë“œ
    waveform, sample_rate = torchaudio.load(audio_path)
    
    # ìµœì†Œ ê¸¸ì´ í™•ì¸ (0.5ì´ˆ)
    min_duration = 0.5
    if end_time - start_time < min_duration:
        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì§§ì€ ê²½ìš°, ì•ë’¤ë¡œ í™•ì¥
        center = (start_time + end_time) / 2
        start_time = max(0, center - min_duration/2)
        end_time = center + min_duration/2
    
    # êµ¬ê°„ ì¶”ì¶œ
    start_sample = int(start_time * sample_rate)
    end_sample = int(end_time * sample_rate)
    segment = waveform[:, start_sample:end_sample]
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        torchaudio.save(tmp.name, segment, sample_rate)
        return tmp.name

def transcribe(audio_path: str) -> str:
    """
    faster-whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    GPU ì—ëŸ¬ ì‹œ CPUë¡œ ìë™ fallbackë©ë‹ˆë‹¤.
    """
    global whisper_model, device, compute_type
    
    try:
        segments, info = whisper_model.transcribe(
            audio_path,   
            language="ko",
            task="transcribe",
            beam_size=2,
            condition_on_previous_text=False,
            # faster-whisper íŠ¹ìœ ì˜ ì„¤ì •ë“¤
            word_timestamps=False,
            vad_filter=True,  # Voice Activity Detection í•„í„° ì‚¬ìš©
            vad_parameters=dict(min_silence_duration_ms=500)
        )
        
        # ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        text_segments = []
        for segment in segments:
            text_segments.append(segment.text)
        
        return " ".join(text_segments).strip()
        
    except Exception as e:
        error_msg = str(e)
        if "cudnn" in error_msg.lower() or "cuda" in error_msg.lower():
            print(f"ğŸ”„ GPU ì—ëŸ¬ ê°ì§€, CPU ëª¨ë“œë¡œ fallback: {error_msg}")
            
            # CPU ëª¨ë“œë¡œ ëª¨ë¸ ì¬ë¡œë“œ
            try:
                device = "cpu"
                compute_type = "int8"
                whisper_model = WhisperModel(WHISPER_MODEL, device=device, compute_type=compute_type)
                print(f"âœ… CPU ëª¨ë“œë¡œ ëª¨ë¸ ì¬ë¡œë“œ ì™„ë£Œ")
                
                # CPU ëª¨ë“œë¡œ ì¬ì‹œë„
                segments, info = whisper_model.transcribe(
                    audio_path,   
                    language="ko",
                    task="transcribe",
                    beam_size=2,
                    condition_on_previous_text=False,
                    word_timestamps=False,
                    vad_filter=True,
                    vad_parameters=dict(min_silence_duration_ms=500)
                )
                
                text_segments = []
                for segment in segments:
                    text_segments.append(segment.text)
                
                return " ".join(text_segments).strip()
                
            except Exception as cpu_error:
                print(f"ğŸ’¥ CPU ëª¨ë“œì—ì„œë„ ì‹¤íŒ¨: {cpu_error}")
                raise cpu_error
        else:
            # GPU/CPUì™€ ê´€ë ¨ì—†ëŠ” ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
            raise e

def process_segment(args: Tuple[str, float, float]) -> str:
    """
    ë‹¨ì¼ ì˜¤ë””ì˜¤ êµ¬ê°„ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        args: (ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ, ì‹œì‘ ì‹œê°„, ì¢…ë£Œ ì‹œê°„) íŠœí”Œ
        
    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        
    Raises:
        ValueError: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
    """
    audio_path, start_time, end_time = args
    
    # ìµœì†Œ ê¸¸ì´ í™•ì¸ (1ì´ˆ)
    min_duration = 1.0  # 0.5ì´ˆì—ì„œ 1ì´ˆë¡œ ì¦ê°€
    if end_time - start_time < min_duration:
        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì§§ì€ ê²½ìš°, ì•ë’¤ë¡œ í™•ì¥
        center = (start_time + end_time) / 2
        start_time = max(0, center - min_duration/2)
        end_time = center + min_duration/2
    
    segment_path = extract_audio_segment(audio_path, start_time, end_time)
    try:
        # ì˜¤ë””ì˜¤ íŒŒì¼ ê²€ì¦
        waveform, sample_rate = torchaudio.load(segment_path)
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
        duration = waveform.shape[1] / sample_rate
        if duration < min_duration:
            raise ValueError(f"Audio segment too short: {duration:.2f}s")
            
        # ì˜¤ë””ì˜¤ í˜•ì‹ ê²€ì¦
        if waveform.shape[0] == 0 or waveform.shape[1] == 0:
            raise ValueError("Invalid audio format: empty waveform")
            
        # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
            
        # ë³¼ë¥¨ ì •ê·œí™”
        waveform = waveform / (torch.max(torch.abs(waveform)) + 1e-8)
        
        # ì •ê·œí™”ëœ ì˜¤ë””ì˜¤ ì €ì¥
        torchaudio.save(segment_path, waveform, sample_rate)
            
        text = transcribe(segment_path)
        text = text.strip()
        if not text:
            raise ValueError("No text extracted from audio segment")
        return text
    finally:
        if os.path.exists(segment_path):
            os.unlink(segment_path)

def transcribe_from_minio(
    key,
    bucket="stream-project-data",
    endpoint_url="http://121.167.129.36:9000",
    access_key="dominic",
    secret_key="gumdong1!530",
    max_workers: int = 4
):
    """
    MinIOì—ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  faster-whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    s3 = boto3.client(
        's3',
        endpoint_url=endpoint_url,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
    )
    obj = s3.get_object(Bucket=bucket, Key=key)
    audio_bytes = obj['Body'].read()
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp:
        tmp.write(audio_bytes)
        tmp.flush()
        tmp_path = tmp.name
        
        try:
            # ìŒì„± êµ¬ê°„ ì¶”ì¶œ
            voice_segments = extract_voice_segments(tmp_path)
            
            if not voice_segments:  # ìŒì„± êµ¬ê°„ì´ ì—†ëŠ” ê²½ìš° fallback ì²˜ë¦¬
                print("âš ï¸ VADë¡œ ìŒì„± êµ¬ê°„ì„ ì°¾ì§€ ëª»í•¨, ì „ì²´ ì˜¤ë””ì˜¤ë¥¼ ì§ì ‘ transcribe ì‹œë„")
                try:
                    # ì „ì²´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì§ì ‘ transcribe
                    full_text = transcribe(tmp_path)
                    if full_text and full_text.strip():
                        print(f"âœ… ì „ì²´ ì˜¤ë””ì˜¤ transcribe ì„±ê³µ: '{full_text[:50]}...'")
                        return full_text.strip()
                    else:
                        print("âŒ ì „ì²´ ì˜¤ë””ì˜¤ì—ì„œë„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•¨")
                        raise ValueError("No text extracted from full audio file")
                except Exception as full_transcribe_error:
                    print(f"ğŸ’¥ ì „ì²´ ì˜¤ë””ì˜¤ transcribe ì‹¤íŒ¨: {full_transcribe_error}")
                    raise ValueError("No voice segments detected and full transcribe failed")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° êµ¬ê°„ ì²˜ë¦¬
            texts = []
            errors = []
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ê° êµ¬ê°„ì— ëŒ€í•´ ì²˜ë¦¬ ì‘ì—… ì œì¶œ
                future_to_segment = {
                    executor.submit(process_segment, (tmp_path, start, end)): (start, end)
                    for start, end in voice_segments
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_segment):
                    start, end = future_to_segment[future]
                    try:
                        text = future.result()
                        texts.append(text)
                    except Exception as e:
                        error_msg = f"Error processing segment {start:.2f}-{end:.2f}: {str(e)}"
                        errors.append(error_msg)
                        print(error_msg)
            
            if not texts:  # ëª¨ë“  êµ¬ê°„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨
                print("âŒ ëª¨ë“  ìŒì„± êµ¬ê°„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨, ì „ì²´ ì˜¤ë””ì˜¤ fallback ì‹œë„")
                try:
                    # ë§ˆì§€ë§‰ ìˆ˜ë‹¨ìœ¼ë¡œ ì „ì²´ ì˜¤ë””ì˜¤ transcribe
                    full_text = transcribe(tmp_path)
                    if full_text and full_text.strip():
                        print(f"âœ… Fallback transcribe ì„±ê³µ: '{full_text[:50]}...'")
                        return full_text.strip()
                    else:
                        raise ValueError("Failed to extract text from any audio segment and full transcribe")
                except Exception as fallback_error:
                    print(f"ğŸ’¥ Fallback transcribeë„ ì‹¤íŒ¨: {fallback_error}")
                    raise ValueError("Failed to extract text from any audio segment and full transcribe")
            
            return " ".join(texts)
            
        finally:
            # ì›ë³¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_path) 